{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TinyZero with A*PO Implementation\n",
        "\n",
        "This notebook implements TinyZero (a reproduction of DeepSeek R1 Zero) using A*PO (Optimal Advantage Regression) instead of GRPO for countdown and multiplication tasks.\n",
        "\n",
        "## Overview\n",
        "- **TinyZero**: A minimal reproduction of DeepSeek R1 Zero focusing on countdown and multiplication tasks\n",
        "- **A*PO**: Optimal Advantage Regression algorithm for efficient policy optimization\n",
        "- **Tasks**: Countdown and multiplication mathematical reasoning\n",
        "- **Framework**: Pure PyTorch with FSDP for training\n",
        "\n",
        "## Requirements\n",
        "- PyTorch for inference and training\n",
        "- PyTorch FSDP for multi-GPU training\n",
        "- Single Python process execution\n",
        "- Modal.com compatibility\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
        "from torch.distributed.fsdp import MixedPrecision\n",
        "from torch.distributed.fsdp import CPUOffload\n",
        "from torch.distributed.fsdp import BackwardPrefetch\n",
        "from torch.distributed.fsdp import ShardingStrategy\n",
        "from torch.distributed.fsdp.wrap import (\n",
        "    size_based_auto_wrap_policy,\n",
        "    enable_wrap,\n",
        "    wrap,\n",
        ")\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "from typing import List, Dict, Tuple, Optional, Any\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "    print(f\"Current GPU: {torch.cuda.current_device()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration and Hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    # Model configuration\n",
        "    vocab_size: int = 32000\n",
        "    d_model: int = 1024\n",
        "    n_heads: int = 16\n",
        "    n_layers: int = 12\n",
        "    d_ff: int = 4096\n",
        "    max_seq_len: int = 512\n",
        "    dropout: float = 0.1\n",
        "    \n",
        "    # Training configuration\n",
        "    batch_size: int = 8\n",
        "    learning_rate: float = 1e-4\n",
        "    num_epochs: int = 10\n",
        "    warmup_steps: int = 1000\n",
        "    max_grad_norm: float = 1.0\n",
        "    \n",
        "    # A*PO specific parameters\n",
        "    num_responses_per_prompt: int = 4  # Number of responses to generate for value estimation\n",
        "    beta1: float = 0.1  # KL regularization coefficient\n",
        "    beta2: float = 0.1  # Advantage regression coefficient\n",
        "    temperature: float = 0.7  # Sampling temperature\n",
        "    \n",
        "    # Task configuration\n",
        "    task_type: str = \"countdown\"  # \"countdown\" or \"multiplication\"\n",
        "    num_train_samples: int = 1000\n",
        "    num_eval_samples: int = 200\n",
        "    \n",
        "    # FSDP configuration\n",
        "    use_fsdp: bool = True\n",
        "    fsdp_cpu_offload: bool = False\n",
        "    fsdp_mixed_precision: bool = True\n",
        "    \n",
        "    # Evaluation configuration\n",
        "    eval_interval: int = 100  # Evaluate every N steps\n",
        "    save_interval: int = 500  # Save checkpoint every N steps\n",
        "\n",
        "config = Config()\n",
        "print(\"Configuration:\")\n",
        "for key, value in config.__dict__.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Task Environments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TaskEnvironment:\n",
        "    \"\"\"Base class for task environments\"\"\"\n",
        "    \n",
        "    def __init__(self, task_type: str):\n",
        "        self.task_type = task_type\n",
        "        self.vocab_size = config.vocab_size\n",
        "        \n",
        "    def generate_sample(self) -> Dict[str, Any]:\n",
        "        \"\"\"Generate a single training sample\"\"\"\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def evaluate_response(self, prompt: str, response: str) -> float:\n",
        "        \"\"\"Evaluate the quality of a response\"\"\"\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def tokenize(self, text: str) -> List[int]:\n",
        "        \"\"\"Simple tokenization (in practice, use a proper tokenizer)\"\"\"\n",
        "        # Simple character-level tokenization for demonstration\n",
        "        return [ord(c) % self.vocab_size for c in text]\n",
        "    \n",
        "    def detokenize(self, tokens: List[int]) -> str:\n",
        "        \"\"\"Simple detokenization\"\"\"\n",
        "        return ''.join([chr(token) for token in tokens if token < 128])\n",
        "\n",
        "\n",
        "class CountdownTask(TaskEnvironment):\n",
        "    \"\"\"Countdown task: Given a target number, use operations to reach it\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__(\"countdown\")\n",
        "        self.operations = ['+', '-', '*', '/']\n",
        "        \n",
        "    def generate_sample(self) -> Dict[str, Any]:\n",
        "        # Generate a random target number\n",
        "        target = random.randint(1, 100)\n",
        "        \n",
        "        # Generate starting numbers\n",
        "        start_nums = [random.randint(1, 20) for _ in range(3)]\n",
        "        \n",
        "        prompt = f\"Target: {target}, Numbers: {start_nums}. Use these numbers and operations to reach the target.\"\n",
        "        \n",
        "        # Generate a correct solution\n",
        "        solution = self._generate_solution(start_nums, target)\n",
        "        \n",
        "        return {\n",
        "            'prompt': prompt,\n",
        "            'target': target,\n",
        "            'start_nums': start_nums,\n",
        "            'solution': solution,\n",
        "            'tokens': self.tokenize(prompt)\n",
        "        }\n",
        "    \n",
        "    def _generate_solution(self, start_nums: List[int], target: int) -> str:\n",
        "        \"\"\"Generate a valid solution for the countdown problem\"\"\"\n",
        "        # Simple brute force to find a solution\n",
        "        for i in range(len(start_nums)):\n",
        "            for j in range(len(start_nums)):\n",
        "                if i != j:\n",
        "                    for op in self.operations:\n",
        "                        try:\n",
        "                            if op == '+':\n",
        "                                result = start_nums[i] + start_nums[j]\n",
        "                            elif op == '-':\n",
        "                                result = start_nums[i] - start_nums[j]\n",
        "                            elif op == '*':\n",
        "                                result = start_nums[i] * start_nums[j]\n",
        "                            elif op == '/':\n",
        "                                if start_nums[j] != 0:\n",
        "                                    result = start_nums[i] / start_nums[j]\n",
        "                                else:\n",
        "                                    continue\n",
        "                            \n",
        "                            if abs(result - target) < 0.01:\n",
        "                                return f\"{start_nums[i]} {op} {start_nums[j]} = {result}\"\n",
        "                        except:\n",
        "                            continue\n",
        "        \n",
        "        # If no exact solution found, return a close approximation\n",
        "        return f\"{start_nums[0]} + {start_nums[1]} = {start_nums[0] + start_nums[1]} (close to {target})\"\n",
        "    \n",
        "    def evaluate_response(self, prompt: str, response: str) -> float:\n",
        "        \"\"\"Evaluate countdown response\"\"\"\n",
        "        # Extract target from prompt\n",
        "        target_match = re.search(r'Target: (\\d+)', prompt)\n",
        "        if not target_match:\n",
        "            return 0.0\n",
        "        \n",
        "        target = int(target_match.group(1))\n",
        "        \n",
        "        # Try to extract result from response\n",
        "        result_match = re.search(r'= (\\d+(?:\\.\\d+)?)', response)\n",
        "        if not result_match:\n",
        "            return 0.0\n",
        "        \n",
        "        try:\n",
        "            result = float(result_match.group(1))\n",
        "            # Reward based on how close the result is to the target\n",
        "            accuracy = max(0, 1.0 - abs(result - target) / max(target, 1))\n",
        "            return accuracy\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "\n",
        "class MultiplicationTask(TaskEnvironment):\n",
        "    \"\"\"Multiplication task: Solve multiplication problems\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__(\"multiplication\")\n",
        "    \n",
        "    def generate_sample(self) -> Dict[str, Any]:\n",
        "        # Generate two random numbers\n",
        "        a = random.randint(1, 99)\n",
        "        b = random.randint(1, 99)\n",
        "        \n",
        "        prompt = f\"What is {a} × {b}?\"\n",
        "        solution = f\"{a} × {b} = {a * b}\"\n",
        "        \n",
        "        return {\n",
        "            'prompt': prompt,\n",
        "            'a': a,\n",
        "            'b': b,\n",
        "            'solution': solution,\n",
        "            'tokens': self.tokenize(prompt)\n",
        "        }\n",
        "    \n",
        "    def evaluate_response(self, prompt: str, response: str) -> float:\n",
        "        \"\"\"Evaluate multiplication response\"\"\"\n",
        "        # Extract numbers from prompt\n",
        "        numbers = re.findall(r'\\d+', prompt)\n",
        "        if len(numbers) < 2:\n",
        "            return 0.0\n",
        "        \n",
        "        a, b = int(numbers[0]), int(numbers[1])\n",
        "        expected = a * b\n",
        "        \n",
        "        # Extract answer from response\n",
        "        answer_match = re.search(r'\\d+', response)\n",
        "        if not answer_match:\n",
        "            return 0.0\n",
        "        \n",
        "        try:\n",
        "            answer = int(answer_match.group(0))\n",
        "            return 1.0 if answer == expected else 0.0\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "\n",
        "# Initialize task environment\n",
        "if config.task_type == \"countdown\":\n",
        "    task_env = CountdownTask()\n",
        "elif config.task_type == \"multiplication\":\n",
        "    task_env = MultiplicationTask()\n",
        "else:\n",
        "    raise ValueError(f\"Unknown task type: {config.task_type}\")\n",
        "\n",
        "print(f\"Initialized {config.task_type} task environment\")\n",
        "\n",
        "# Test task environment\n",
        "sample = task_env.generate_sample()\n",
        "print(f\"Sample prompt: {sample['prompt']}\")\n",
        "print(f\"Sample solution: {sample['solution']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Positional encoding for transformer\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        \n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        \n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Single transformer block\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        \n",
        "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        \n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        # Self-attention\n",
        "        attn_output, _ = self.self_attn(x, x, x, attn_mask=mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        \n",
        "        # Feed forward\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class TinyZeroModel(nn.Module):\n",
        "    \"\"\"TinyZero model architecture\"\"\"\n",
        "    \n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        \n",
        "        # Embedding layers\n",
        "        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.pos_encoding = PositionalEncoding(config.d_model, config.max_seq_len)\n",
        "        \n",
        "        # Transformer blocks\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(config.d_model, config.n_heads, config.d_ff, config.dropout)\n",
        "            for _ in range(config.n_layers)\n",
        "        ])\n",
        "        \n",
        "        # Output layers\n",
        "        self.ln_f = nn.LayerNorm(config.d_model)\n",
        "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "        \n",
        "        # Value head for A*PO\n",
        "        self.value_head = nn.Linear(config.d_model, 1)\n",
        "        \n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask=None, return_value=False):\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        \n",
        "        # Token embeddings\n",
        "        x = self.token_embedding(input_ids)\n",
        "        x = self.pos_encoding(x.transpose(0, 1)).transpose(0, 1)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # Create causal mask\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(batch_size, seq_len, device=input_ids.device)\n",
        "        \n",
        "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=input_ids.device), diagonal=1).bool()\n",
        "        \n",
        "        # Apply transformer blocks\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, mask=causal_mask)\n",
        "        \n",
        "        # Final layer norm\n",
        "        x = self.ln_f(x)\n",
        "        \n",
        "        # Language modeling head\n",
        "        logits = self.lm_head(x)\n",
        "        \n",
        "        if return_value:\n",
        "            # Value estimation (use last token)\n",
        "            values = self.value_head(x[:, -1, :])  # [batch_size, 1]\n",
        "            return logits, values\n",
        "        \n",
        "        return logits\n",
        "    \n",
        "    def generate(self, input_ids, max_length=100, temperature=1.0, do_sample=True):\n",
        "        \"\"\"Generate text using the model\"\"\"\n",
        "        self.eval()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_length):\n",
        "                logits = self.forward(input_ids)\n",
        "                next_token_logits = logits[:, -1, :] / temperature\n",
        "                \n",
        "                if do_sample:\n",
        "                    probs = F.softmax(next_token_logits, dim=-1)\n",
        "                    next_token = torch.multinomial(probs, num_samples=1)\n",
        "                else:\n",
        "                    next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
        "                \n",
        "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "                \n",
        "                # Stop if we hit a special token (simplified)\n",
        "                if next_token.item() == 0:  # Assuming 0 is a stop token\n",
        "                    break\n",
        "        \n",
        "        return input_ids\n",
        "\n",
        "\n",
        "# Initialize model\n",
        "model = TinyZeroModel(config)\n",
        "model = model.to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Model initialized on {device}\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Model size: {total_params * 4 / 1024 / 1024:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. A*PO Algorithm Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AstarPO:\n",
        "    \"\"\"A*PO (Optimal Advantage Regression) algorithm\"\"\"\n",
        "    \n",
        "    def __init__(self, model, config: Config):\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
        "        \n",
        "        # Reference model for value estimation (frozen copy)\n",
        "        self.reference_model = TinyZeroModel(config)\n",
        "        self.reference_model.load_state_dict(model.state_dict())\n",
        "        self.reference_model.eval()\n",
        "        \n",
        "        # Freeze reference model parameters\n",
        "        for param in self.reference_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        \n",
        "        self.reference_model = self.reference_model.to(device)\n",
        "        \n",
        "        # Training statistics\n",
        "        self.step_count = 0\n",
        "        self.loss_history = []\n",
        "        \n",
        "    def generate_responses(self, prompts: List[str], num_responses: int = None) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Generate multiple responses for each prompt using reference model\"\"\"\n",
        "        if num_responses is None:\n",
        "            num_responses = self.config.num_responses_per_prompt\n",
        "        \n",
        "        responses = []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for prompt in prompts:\n",
        "                prompt_tokens = task_env.tokenize(prompt)\n",
        "                input_ids = torch.tensor([prompt_tokens], device=device)\n",
        "                \n",
        "                prompt_responses = []\n",
        "                \n",
        "                for _ in range(num_responses):\n",
        "                    # Generate response\n",
        "                    generated_ids = self.reference_model.generate(\n",
        "                        input_ids, \n",
        "                        max_length=50, \n",
        "                        temperature=self.config.temperature,\n",
        "                        do_sample=True\n",
        "                    )\n",
        "                    \n",
        "                    # Extract response tokens (excluding prompt)\n",
        "                    response_tokens = generated_ids[0, len(prompt_tokens):].tolist()\n",
        "                    response_text = task_env.detokenize(response_tokens)\n",
        "                    \n",
        "                    # Evaluate response\n",
        "                    reward = task_env.evaluate_response(prompt, response_text)\n",
        "                    \n",
        "                    prompt_responses.append({\n",
        "                        'prompt': prompt,\n",
        "                        'response': response_text,\n",
        "                        'tokens': response_tokens,\n",
        "                        'reward': reward\n",
        "                    })\n",
        "                \n",
        "                responses.extend(prompt_responses)\n",
        "        \n",
        "        return responses\n",
        "    \n",
        "    def estimate_optimal_values(self, responses: List[Dict[str, Any]]) -> torch.Tensor:\n",
        "        \"\"\"Estimate optimal values using reference model\"\"\"\n",
        "        values = []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for response_data in responses:\n",
        "                # Combine prompt and response\n",
        "                full_text = response_data['prompt'] + ' ' + response_data['response']\n",
        "                tokens = task_env.tokenize(full_text)\n",
        "                \n",
        "                if len(tokens) == 0:\n",
        "                    values.append(0.0)\n",
        "                    continue\n",
        "                \n",
        "                input_ids = torch.tensor([tokens], device=device)\n",
        "                \n",
        "                # Get value estimate from reference model\n",
        "                _, value = self.reference_model(input_ids, return_value=True)\n",
        "                values.append(value.item())\n",
        "        \n",
        "        return torch.tensor(values, device=device)\n",
        "    \n",
        "    def compute_advantages(self, rewards: torch.Tensor, values: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Compute advantages using rewards and value estimates\"\"\"\n",
        "        # Simple advantage computation: A = R - V\n",
        "        advantages = rewards - values\n",
        "        return advantages\n",
        "    \n",
        "    def compute_kl_divergence(self, logits_new: torch.Tensor, logits_ref: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Compute KL divergence between new and reference policies\"\"\"\n",
        "        probs_new = F.softmax(logits_new, dim=-1)\n",
        "        log_probs_new = F.log_softmax(logits_new, dim=-1)\n",
        "        log_probs_ref = F.log_softmax(logits_ref, dim=-1)\n",
        "        \n",
        "        kl_div = F.kl_div(log_probs_new, probs_new, reduction='none').sum(dim=-1)\n",
        "        return kl_div\n",
        "    \n",
        "    def update(self, batch_data: List[Dict[str, Any]]) -> Dict[str, float]:\n",
        "        \"\"\"Perform A*PO update step\"\"\"\n",
        "        self.model.train()\n",
        "        \n",
        "        # Extract prompts\n",
        "        prompts = [data['prompt'] for data in batch_data]\n",
        "        \n",
        "        # Generate multiple responses for value estimation\n",
        "        responses = self.generate_responses(prompts)\n",
        "        \n",
        "        # Extract rewards and estimate values\n",
        "        rewards = torch.tensor([r['reward'] for r in responses], device=device)\n",
        "        optimal_values = self.estimate_optimal_values(responses)\n",
        "        \n",
        "        # Compute advantages\n",
        "        advantages = self.compute_advantages(rewards, optimal_values)\n",
        "        \n",
        "        # Prepare training data\n",
        "        total_loss = 0.0\n",
        "        kl_loss = 0.0\n",
        "        value_loss = 0.0\n",
        "        \n",
        "        for i, response_data in enumerate(responses):\n",
        "            # Get full sequence (prompt + response)\n",
        "            full_text = response_data['prompt'] + ' ' + response_data['response']\n",
        "            tokens = task_env.tokenize(full_text)\n",
        "            \n",
        "            if len(tokens) == 0:\n",
        "                continue\n",
        "            \n",
        "            input_ids = torch.tensor([tokens], device=device)\n",
        "            \n",
        "            # Forward pass through current model\n",
        "            logits_new, values_new = self.model(input_ids, return_value=True)\n",
        "            \n",
        "            # Forward pass through reference model\n",
        "            with torch.no_grad():\n",
        "                logits_ref, values_ref = self.reference_model(input_ids, return_value=True)\n",
        "            \n",
        "            # Compute KL divergence\n",
        "            kl_div = self.compute_kl_divergence(logits_new, logits_ref)\n",
        "            \n",
        "            # Compute losses\n",
        "            advantage = advantages[i]\n",
        "            \n",
        "            # Policy loss (advantage-weighted log probability)\n",
        "            log_probs = F.log_softmax(logits_new, dim=-1)\n",
        "            policy_loss = -advantage * log_probs.mean()\n",
        "            \n",
        "            # Value loss (MSE between predicted and optimal values)\n",
        "            value_loss_item = F.mse_loss(values_new.squeeze(), optimal_values[i])\n",
        "            \n",
        "            # KL regularization\n",
        "            kl_loss_item = self.config.beta1 * kl_div.mean()\n",
        "            \n",
        "            # Total loss\n",
        "            loss = policy_loss + value_loss_item + kl_loss_item\n",
        "            \n",
        "            total_loss += loss\n",
        "            kl_loss += kl_loss_item\n",
        "            value_loss += value_loss_item\n",
        "        \n",
        "        # Average losses\n",
        "        num_samples = len(responses)\n",
        "        if num_samples > 0:\n",
        "            total_loss /= num_samples\n",
        "            kl_loss /= num_samples\n",
        "            value_loss /= num_samples\n",
        "        \n",
        "        # Backward pass\n",
        "        self.optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        \n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
        "        \n",
        "        self.optimizer.step()\n",
        "        \n",
        "        # Update step count\n",
        "        self.step_count += 1\n",
        "        \n",
        "        # Update reference model periodically\n",
        "        if self.step_count % 100 == 0:\n",
        "            self.reference_model.load_state_dict(self.model.state_dict())\n",
        "        \n",
        "        # Record losses\n",
        "        loss_dict = {\n",
        "            'total_loss': total_loss.item(),\n",
        "            'kl_loss': kl_loss.item(),\n",
        "            'value_loss': value_loss.item(),\n",
        "            'mean_reward': rewards.mean().item(),\n",
        "            'mean_advantage': advantages.mean().item()\n",
        "        }\n",
        "        \n",
        "        self.loss_history.append(loss_dict)\n",
        "        \n",
        "        return loss_dict\n",
        "\n",
        "\n",
        "# Initialize A*PO trainer\n",
        "trainer = AstarPO(model, config)\n",
        "print(\"A*PO trainer initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Data Generation and Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_dataset(num_samples: int, task_env: TaskEnvironment) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Generate training dataset\"\"\"\n",
        "    dataset = []\n",
        "    \n",
        "    print(f\"Generating {num_samples} samples for {task_env.task_type} task...\")\n",
        "    \n",
        "    for i in range(num_samples):\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"Generated {i + 1}/{num_samples} samples\")\n",
        "        \n",
        "        sample = task_env.generate_sample()\n",
        "        dataset.append(sample)\n",
        "    \n",
        "    print(f\"Dataset generation complete: {len(dataset)} samples\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def create_data_loader(dataset: List[Dict[str, Any]], batch_size: int, shuffle: bool = True):\n",
        "    \"\"\"Create data loader for training\"\"\"\n",
        "    def collate_fn(batch):\n",
        "        return batch  # Return as-is for now\n",
        "    \n",
        "    # Simple batch creation\n",
        "    batches = []\n",
        "    for i in range(0, len(dataset), batch_size):\n",
        "        batch = dataset[i:i + batch_size]\n",
        "        batches.append(batch)\n",
        "    \n",
        "    if shuffle:\n",
        "        random.shuffle(batches)\n",
        "    \n",
        "    return batches\n",
        "\n",
        "\n",
        "def evaluate_model(model, eval_batches: List[List[Dict[str, Any]]], task_env: TaskEnvironment) -> Dict[str, float]:\n",
        "    \"\"\"Evaluate model performance\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    total_rewards = 0.0\n",
        "    total_samples = 0\n",
        "    correct_predictions = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in eval_batches:\n",
        "            for sample in batch:\n",
        "                prompt = sample['prompt']\n",
        "                \n",
        "                # Generate response\n",
        "                prompt_tokens = task_env.tokenize(prompt)\n",
        "                input_ids = torch.tensor([prompt_tokens], device=device)\n",
        "                \n",
        "                generated_ids = model.generate(\n",
        "                    input_ids, \n",
        "                    max_length=50, \n",
        "                    temperature=0.1,  # Low temperature for evaluation\n",
        "                    do_sample=False   # Greedy decoding\n",
        "                )\n",
        "                \n",
        "                # Extract response\n",
        "                response_tokens = generated_ids[0, len(prompt_tokens):].tolist()\n",
        "                response_text = task_env.detokenize(response_tokens)\n",
        "                \n",
        "                # Evaluate response\n",
        "                reward = task_env.evaluate_response(prompt, response_text)\n",
        "                \n",
        "                total_rewards += reward\n",
        "                total_samples += 1\n",
        "                \n",
        "                if reward > 0.9:  # Consider correct if reward > 0.9\n",
        "                    correct_predictions += 1\n",
        "    \n",
        "    accuracy = correct_predictions / total_samples if total_samples > 0 else 0.0\n",
        "    avg_reward = total_rewards / total_samples if total_samples > 0 else 0.0\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'avg_reward': avg_reward,\n",
        "        'total_samples': total_samples,\n",
        "        'correct_predictions': correct_predictions\n",
        "    }\n",
        "\n",
        "\n",
        "def train_model(trainer: AstarPO, train_batches: List[List[Dict[str, Any]]], \n",
        "                eval_batches: List[List[Dict[str, Any]]], config: Config):\n",
        "    \"\"\"Main training loop\"\"\"\n",
        "    print(f\"Starting training for {config.num_epochs} epochs...\")\n",
        "    print(f\"Total training batches: {len(train_batches)}\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    for epoch in range(config.num_epochs):\n",
        "        epoch_start_time = time.time()\n",
        "        epoch_losses = []\n",
        "        \n",
        "        print(f\"\\nEpoch {epoch + 1}/{config.num_epochs}\")\n",
        "        \n",
        "        for batch_idx, batch in enumerate(train_batches):\n",
        "            # Training step\n",
        "            loss_dict = trainer.update(batch)\n",
        "            epoch_losses.append(loss_dict)\n",
        "            \n",
        "            # Log progress\n",
        "            if (batch_idx + 1) % 10 == 0:\n",
        "                avg_loss = np.mean([l['total_loss'] for l in epoch_losses[-10:]])\n",
        "                avg_reward = np.mean([l['mean_reward'] for l in epoch_losses[-10:]])\n",
        "                print(f\"  Batch {batch_idx + 1}/{len(train_batches)}: Loss={avg_loss:.4f}, Reward={avg_reward:.4f}\")\n",
        "            \n",
        "            # Evaluation\n",
        "            if trainer.step_count % config.eval_interval == 0:\n",
        "                print(f\"\\nEvaluating at step {trainer.step_count}...\")\n",
        "                eval_results = evaluate_model(trainer.model, eval_batches, task_env)\n",
        "                print(f\"Evaluation results: {eval_results}\")\n",
        "            \n",
        "            # Save checkpoint\n",
        "            if trainer.step_count % config.save_interval == 0:\n",
        "                checkpoint = {\n",
        "                    'model_state_dict': trainer.model.state_dict(),\n",
        "                    'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
        "                    'step_count': trainer.step_count,\n",
        "                    'config': config,\n",
        "                    'loss_history': trainer.loss_history\n",
        "                }\n",
        "                torch.save(checkpoint, f'checkpoint_step_{trainer.step_count}.pt')\n",
        "                print(f\"Checkpoint saved at step {trainer.step_count}\")\n",
        "        \n",
        "        # Epoch summary\n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "        avg_epoch_loss = np.mean([l['total_loss'] for l in epoch_losses])\n",
        "        avg_epoch_reward = np.mean([l['mean_reward'] for l in epoch_losses])\n",
        "        \n",
        "        print(f\"\\nEpoch {epoch + 1} completed in {epoch_time:.2f}s\")\n",
        "        print(f\"Average loss: {avg_epoch_loss:.4f}\")\n",
        "        print(f\"Average reward: {avg_epoch_reward:.4f}\")\n",
        "        \n",
        "        # Final evaluation\n",
        "        print(f\"\\nFinal evaluation for epoch {epoch + 1}...\")\n",
        "        eval_results = evaluate_model(trainer.model, eval_batches, task_env)\n",
        "        print(f\"Final evaluation results: {eval_results}\")\n",
        "    \n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\nTraining completed in {total_time:.2f}s\")\n",
        "    \n",
        "    return trainer.loss_history\n",
        "\n",
        "\n",
        "# Generate datasets\n",
        "print(\"Generating training dataset...\")\n",
        "train_dataset = generate_dataset(config.num_train_samples, task_env)\n",
        "\n",
        "print(\"Generating evaluation dataset...\")\n",
        "eval_dataset = generate_dataset(config.num_eval_samples, task_env)\n",
        "\n",
        "# Create data loaders\n",
        "train_batches = create_data_loader(train_dataset, config.batch_size, shuffle=True)\n",
        "eval_batches = create_data_loader(eval_dataset, config.batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Training batches: {len(train_batches)}\")\n",
        "print(f\"Evaluation batches: {len(eval_batches)}\")\n",
        "print(f\"Batch size: {config.batch_size}\")\n",
        "\n",
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "loss_history = train_model(trainer, train_batches, eval_batches, config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Results Analysis and Visualization\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Final Evaluation and Model Saving\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final comprehensive evaluation\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINAL EVALUATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Evaluate on test set\n",
        "final_eval_results = evaluate_model(trainer.model, eval_batches, task_env)\n",
        "print(f\"\\nFinal Evaluation Results:\")\n",
        "for key, value in final_eval_results.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Test on a few specific examples\n",
        "print(f\"\\nTesting on specific examples:\")\n",
        "test_samples = eval_dataset[:5]  # Test on first 5 samples\n",
        "\n",
        "for i, sample in enumerate(test_samples):\n",
        "    print(f\"\\nExample {i + 1}:\")\n",
        "    print(f\"Prompt: {sample['prompt']}\")\n",
        "    print(f\"Expected: {sample['solution']}\")\n",
        "    \n",
        "    # Generate response\n",
        "    prompt_tokens = task_env.tokenize(sample['prompt'])\n",
        "    input_ids = torch.tensor([prompt_tokens], device=device)\n",
        "    \n",
        "    generated_ids = trainer.model.generate(\n",
        "        input_ids, \n",
        "        max_length=50, \n",
        "        temperature=0.1,\n",
        "        do_sample=False\n",
        "    )\n",
        "    \n",
        "    response_tokens = generated_ids[0, len(prompt_tokens):].tolist()\n",
        "    response_text = task_env.detokenize(response_tokens)\n",
        "    \n",
        "    reward = task_env.evaluate_response(sample['prompt'], response_text)\n",
        "    \n",
        "    print(f\"Generated: {response_text}\")\n",
        "    print(f\"Reward: {reward:.4f}\")\n",
        "\n",
        "# Model statistics\n",
        "print(f\"\\nModel Statistics:\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in trainer.model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in trainer.model.parameters() if p.requires_grad):,}\")\n",
        "print(f\"Model size: {sum(p.numel() for p in trainer.model.parameters()) * 4 / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "# Training statistics\n",
        "if loss_history:\n",
        "    print(f\"\\nTraining Statistics:\")\n",
        "    print(f\"Total training steps: {len(loss_history)}\")\n",
        "    print(f\"Total epochs: {config.num_epochs}\")\n",
        "    print(f\"Batch size: {config.batch_size}\")\n",
        "    print(f\"Learning rate: {config.learning_rate}\")\n",
        "    print(f\"A*PO parameters:\")\n",
        "    print(f\"  - Number of responses per prompt: {config.num_responses_per_prompt}\")\n",
        "    print(f\"  - Beta1 (KL regularization): {config.beta1}\")\n",
        "    print(f\"  - Beta2 (Advantage regression): {config.beta2}\")\n",
        "    print(f\"  - Temperature: {config.temperature}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAINING COMPLETED SUCCESSFULLY\")\n",
        "print(\"=\"*50)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
